
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   
      <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      -->
      <title>Underdetermined Problem</title>
      <meta name="generator" content="MATLAB 7.8">
      <meta name="date" content="2009-01-19">
      <meta name="m-file" content="demolin5">
      <link rel="stylesheet" type="text/css" href="../../../matlab/demos/private/style.css">
   </head>
   <body>
      <div class="header">
         <div class="left"><a href="matlab:edit demolin5">Open demolin5.m in the Editor</a></div>
         <div class="right"><a href="matlab:echodemo demolin5">Run in the Command Window</a></div>
      </div>
      <div class="content">
         <h1>Underdetermined Problem</h1>
         <!--introduction-->
         <p>A linear neuron is trained to find a non-unique solution to an undetermined problem.</p>
         <!--/introduction-->
         <p>P defines one 1-element input patterns (column vectors).  T defines an associated 1-element target (column vectors).  Note
            that there are infinite values of W and B such that the expression W*P+B = T is true.  Problems with multiple solutions are
            called underdetermined.
         </p><pre class="codeinput">P = [+1.0];
T = [+0.5];
</pre><p>ERRSURF calculates errors for a neuron with a range of possible weight and bias values.  PLOTES plots this error surface with
            a contour plot underneath. The bottom of the valley in the error surface corresponds to the infinite solutions to this problem.
         </p><pre class="codeinput">w_range = -1:0.2:1;  b_range = -1:0.2:1;
ES = errsurf(P,T,w_range,b_range,<span class="string">'purelin'</span>);
plotes(w_range,b_range,ES);
</pre><img vspace="5" hspace="5" src="demolin5_01.png" alt=""> <p>MAXLINLR finds the fastest stable learning rate for training a linear network. NEWLIN creates a linear neuron.  NEWLIN takes
            these arguments: 1) Rx2 matrix of min and max values for R input elements, 2) Number of elements in the output vector, 3)
            Input delay vector, and 4) Learning rate.
         </p><pre class="codeinput">maxlr = maxlinlr(P,<span class="string">'bias'</span>);
net = newlin([-2 2],1,[0],maxlr);
</pre><p>Override the default training parameters by setting the performance goal.</p><pre class="codeinput">net.trainParam.goal = 1e-10;
</pre><p>To show the path of the training we will train only one epoch at a time and call PLOTEP every epoch.  The plot shows a history
            of the training.  Each dot represents an epoch and the blue lines show each change made by the learning rule (Widrow-Hoff
            by default).
         </p><pre class="codeinput"><span class="comment">% [net,tr] = train(net,P,T);</span>
net.trainParam.epochs = 1;
net.trainParam.show = NaN;
h=plotep(net.IW{1},net.b{1},mse(T-sim(net,P)));
[net,tr] = train(net,P,T);
r = tr;
epoch = 1;
<span class="keyword">while</span> true
   epoch = epoch+1;
   [net,tr] = train(net,P,T);
   <span class="keyword">if</span> length(tr.epoch) &gt; 1
      h = plotep(net.IW{1,1},net.b{1},tr.perf(2),h);
      r.epoch=[r.epoch epoch];
      r.perf=[r.perf tr.perf(2)];
      r.vperf=[r.vperf NaN];
      r.tperf=[r.tperf NaN];
   <span class="keyword">else</span>
      <span class="keyword">break</span>
   <span class="keyword">end</span>
<span class="keyword">end</span>
tr=r;
</pre><img vspace="5" hspace="5" src="demolin5_02.png" alt=""> <p>Here we plot the NEWLIND solution.  Note that the TRAIN (white dot) and SOLVELIN (red circle) solutions are not the same.
             In fact, TRAINWH will return a different solution for different initial conditions, while SOLVELIN will always return the
            same solution.
         </p><pre class="codeinput">solvednet = newlind(P,T);
hold <span class="string">on</span>;
plot(solvednet.IW{1,1},solvednet.b{1},<span class="string">'ro'</span>)
hold <span class="string">off</span>;
</pre><img vspace="5" hspace="5" src="demolin5_03.png" alt=""> <p>The train function outputs the trained network and a history of the training performance (tr).  Here the errors are plotted
            with respect to training epochs: Once the error reaches the goal, an adequate solution for W and B has been found.  However,
            because the problem is underdetermined, this solution is not unique.
         </p><pre class="codeinput">subplot(1,2,1);
plotperf(tr,net.trainParam.goal);
</pre><img vspace="5" hspace="5" src="demolin5_04.png" alt=""> <p>We can now test the associator with one of the original inputs, 1.0, and see if it returns the target, 0.5.  The result is
            very close to 0.5.  The error can be reduced further, if required, by continued training with TRAINWH using a smaller error
            goal.
         </p><pre class="codeinput">p = 1.0;
a = sim(net,p)
</pre><pre class="codeoutput">
a =

    0.5000

</pre><p class="footer">Copyright 1992-2005 The MathWorks, Inc.<br>
            Published with MATLAB&reg; 7.8
         </p>
         <p class="footer" id="trademarks">MATLAB and Simulink are registered trademarks of The MathWorks, Inc.  Please see <a href="http://www.mathworks.com/trademarks">www.mathworks.com/trademarks</a> for a list of other trademarks owned by The MathWorks, Inc.  Other product or brand names are trademarks or registered trademarks
            of their respective owners.
         </p>
      </div>
      <!--
##### SOURCE BEGIN #####
%% Underdetermined Problem
% A linear neuron is trained to find a non-unique solution to an undetermined
% problem.
%
% Copyright 1992-2005 The MathWorks, Inc.
% $Revision: 1.12.2.1 $  $Date: 2005/11/15 01:15:10 $

%%
% P defines one 1-element input patterns (column vectors).  T defines an
% associated 1-element target (column vectors).  Note that there are infinite
% values of W and B such that the expression W*P+B = T is true.  Problems with
% multiple solutions are called underdetermined.

P = [+1.0];
T = [+0.5];

%%
% ERRSURF calculates errors for a neuron with a range of possible weight and
% bias values.  PLOTES plots this error surface with a contour plot underneath.
% The bottom of the valley in the error surface corresponds to the infinite
% solutions to this problem.

w_range = -1:0.2:1;  b_range = -1:0.2:1;
ES = errsurf(P,T,w_range,b_range,'purelin');
plotes(w_range,b_range,ES);

%%
% MAXLINLR finds the fastest stable learning rate for training a linear network.
% NEWLIN creates a linear neuron.  NEWLIN takes these arguments: 1) Rx2 matrix
% of min and max values for R input elements, 2) Number of elements in the
% output vector, 3) Input delay vector, and 4) Learning rate.

maxlr = maxlinlr(P,'bias');
net = newlin([-2 2],1,[0],maxlr);


%%
% Override the default training parameters by setting the performance goal.

net.trainParam.goal = 1e-10;

%%
% To show the path of the training we will train only one epoch at a time and
% call PLOTEP every epoch.  The plot shows a history of the training.  Each dot
% represents an epoch and the blue lines show each change made by the learning
% rule (Widrow-Hoff by default).

% [net,tr] = train(net,P,T);
net.trainParam.epochs = 1;
net.trainParam.show = NaN;
h=plotep(net.IW{1},net.b{1},mse(T-sim(net,P)));     
[net,tr] = train(net,P,T);                                                    
r = tr;
epoch = 1;
while true
   epoch = epoch+1;
   [net,tr] = train(net,P,T);
   if length(tr.epoch) > 1
      h = plotep(net.IW{1,1},net.b{1},tr.perf(2),h);
      r.epoch=[r.epoch epoch]; 
      r.perf=[r.perf tr.perf(2)];
      r.vperf=[r.vperf NaN];
      r.tperf=[r.tperf NaN];
   else
      break
   end
end
tr=r;

%%
% Here we plot the NEWLIND solution.  Note that the TRAIN (white dot) and
% SOLVELIN (red circle) solutions are not the same.  In fact, TRAINWH will
% return a different solution for different initial conditions, while SOLVELIN
% will always return the same solution.

solvednet = newlind(P,T);
hold on;
plot(solvednet.IW{1,1},solvednet.b{1},'ro')
hold off;

%%
% The train function outputs the trained network and a history of the training
% performance (tr).  Here the errors are plotted with respect to training
% epochs: Once the error reaches the goal, an adequate solution for W and B has
% been found.  However, because the problem is underdetermined, this solution is
% not unique.

subplot(1,2,1);
plotperf(tr,net.trainParam.goal);


%%
% We can now test the associator with one of the original inputs, 1.0, and see
% if it returns the target, 0.5.  The result is very close to 0.5.  The error
% can be reduced further, if required, by continued training with TRAINWH using
% a smaller error goal.

p = 1.0;
a = sim(net,p)


displayEndOfDemoMessage(mfilename)
##### SOURCE END #####
-->
   </body>
</html>