
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   
      <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      -->
      <title>Radial Basis Approximation</title>
      <meta name="generator" content="MATLAB 7.8">
      <meta name="date" content="2009-01-19">
      <meta name="m-file" content="demorb1">
      <link rel="stylesheet" type="text/css" href="../../../matlab/demos/private/style.css">
   </head>
   <body>
      <div class="header">
         <div class="left"><a href="matlab:edit demorb1">Open demorb1.m in the Editor</a></div>
         <div class="right"><a href="matlab:echodemo demorb1">Run in the Command Window</a></div>
      </div>
      <div class="content">
         <h1>Radial Basis Approximation</h1>
         <!--introduction-->
         <p>This demo uses the NEWRB function to create a radial basis network that approximates a function defined by a set of data points.</p>
         <!--/introduction-->
         <p>Define 21 inputs P and associated targets T.</p><pre class="codeinput">P = -1:.1:1;
T = [-.9602 -.5770 -.0729  .3771  .6405  .6600  .4609 <span class="keyword">...</span>
      .1336 -.2013 -.4344 -.5000 -.3930 -.1647  .0988 <span class="keyword">...</span>
      .3072  .3960  .3449  .1816 -.0312 -.2189 -.3201];
plot(P,T,<span class="string">'+'</span>);
title(<span class="string">'Training Vectors'</span>);
xlabel(<span class="string">'Input Vector P'</span>);
ylabel(<span class="string">'Target Vector T'</span>);
</pre><img vspace="5" hspace="5" src="demorb1_01.png" alt=""> <p>We would like to find a function which fits the 21 data points.  One way to do this is with a radial basis network.  A radial
            basis network is a network with two layers.  A hidden layer of radial basis neurons and an output layer of linear neurons.
             Here is the radial basis transfer function used by the hidden layer.
         </p><pre class="codeinput">p = -3:.1:3;
a = radbas(p);
plot(p,a)
title(<span class="string">'Radial Basis Transfer Function'</span>);
xlabel(<span class="string">'Input p'</span>);
ylabel(<span class="string">'Output a'</span>);
</pre><img vspace="5" hspace="5" src="demorb1_02.png" alt=""> <p>The weights and biases of each neuron in the hidden layer define the position and width of a radial basis function.  Each
            linear output neuron forms a weighted sum of these radial basis functions.  With the correct weight and bias values for each
            layer, and enough hidden neurons, a radial basis network can fit any function with any desired accuracy.  This is an example
            of three radial basis functions (in blue) are scaled and summed to produce a function (in magenta).
         </p><pre class="codeinput">a2 = radbas(p-1.5);
a3 = radbas(p+2);
a4 = a + a2*1 + a3*0.5;
plot(p,a,<span class="string">'b-'</span>,p,a2,<span class="string">'b--'</span>,p,a3,<span class="string">'b--'</span>,p,a4,<span class="string">'m-'</span>)
title(<span class="string">'Weighted Sum of Radial Basis Transfer Functions'</span>);
xlabel(<span class="string">'Input p'</span>);
ylabel(<span class="string">'Output a'</span>);
</pre><img vspace="5" hspace="5" src="demorb1_03.png" alt=""> <p>The function NEWRB quickly creates a radial basis network which approximates the function defined by P and T.  In addition
            to the training set and targets, NEWRB takes two arguments, the sum-squared error goal and the spread constant.
         </p><pre class="codeinput">eg = 0.02; <span class="comment">% sum-squared error goal</span>
sc = 1;    <span class="comment">% spread constant</span>
net = newrb(P,T,eg,sc);
</pre><pre class="codeoutput">NEWRB, neurons = 0, MSE = 0.176192
</pre><p>To see how the network performs, replot the training set.  Then simulate the network response for inputs over the same range.
             Finally, plot the results on the same graph.
         </p><pre class="codeinput">plot(P,T,<span class="string">'+'</span>);
xlabel(<span class="string">'Input'</span>);

X = -1:.01:1;
Y = sim(net,X);

hold <span class="string">on</span>;
plot(X,Y);
hold <span class="string">off</span>;
legend({<span class="string">'Target'</span>,<span class="string">'Output'</span>})
</pre><img vspace="5" hspace="5" src="demorb1_04.png" alt=""> <p class="footer">Copyright 1992-2005 The MathWorks, Inc.<br>
            Published with MATLAB&reg; 7.8
         </p>
         <p class="footer" id="trademarks">MATLAB and Simulink are registered trademarks of The MathWorks, Inc.  Please see <a href="http://www.mathworks.com/trademarks">www.mathworks.com/trademarks</a> for a list of other trademarks owned by The MathWorks, Inc.  Other product or brand names are trademarks or registered trademarks
            of their respective owners.
         </p>
      </div>
      <!--
##### SOURCE BEGIN #####
%% Radial Basis Approximation
% This demo uses the NEWRB function to create a radial basis network that
% approximates a function defined by a set of data points.
%
% Copyright 1992-2005 The MathWorks, Inc.
% $Revision: 1.14.2.1 $  $Date: 2005/11/15 01:15:20 $

%%
% Define 21 inputs P and associated targets T.

P = -1:.1:1;
T = [-.9602 -.5770 -.0729  .3771  .6405  .6600  .4609 ...
      .1336 -.2013 -.4344 -.5000 -.3930 -.1647  .0988 ...
      .3072  .3960  .3449  .1816 -.0312 -.2189 -.3201];
plot(P,T,'+');
title('Training Vectors');
xlabel('Input Vector P');
ylabel('Target Vector T');

%%
% We would like to find a function which fits the 21 data points.  One way to do
% this is with a radial basis network.  A radial basis network is a network with
% two layers.  A hidden layer of radial basis neurons and an output layer of
% linear neurons.  Here is the radial basis transfer function used by the hidden
% layer.

p = -3:.1:3;
a = radbas(p);
plot(p,a)
title('Radial Basis Transfer Function');
xlabel('Input p');
ylabel('Output a');

%%
% The weights and biases of each neuron in the hidden layer define the position
% and width of a radial basis function.  Each linear output neuron forms a
% weighted sum of these radial basis functions.  With the correct weight and
% bias values for each layer, and enough hidden neurons, a radial basis network
% can fit any function with any desired accuracy.  This is an example of three
% radial basis functions (in blue) are scaled and summed to produce a function
% (in magenta).

a2 = radbas(p-1.5);
a3 = radbas(p+2);
a4 = a + a2*1 + a3*0.5;
plot(p,a,'b-',p,a2,'bREPLACE_WITH_DASH_DASH',p,a3,'bREPLACE_WITH_DASH_DASH',p,a4,'m-')
title('Weighted Sum of Radial Basis Transfer Functions');
xlabel('Input p');
ylabel('Output a');

%%
% The function NEWRB quickly creates a radial basis network which approximates
% the function defined by P and T.  In addition to the training set and targets,
% NEWRB takes two arguments, the sum-squared error goal and the spread constant.

eg = 0.02; % sum-squared error goal
sc = 1;    % spread constant
net = newrb(P,T,eg,sc);

%%
% To see how the network performs, replot the training set.  Then simulate the
% network response for inputs over the same range.  Finally, plot the results on
% the same graph.

plot(P,T,'+');
xlabel('Input');

X = -1:.01:1;
Y = sim(net,X);

hold on;
plot(X,Y);
hold off;
legend({'Target','Output'})


displayEndOfDemoMessage(mfilename)
##### SOURCE END #####
-->
   </body>
</html>